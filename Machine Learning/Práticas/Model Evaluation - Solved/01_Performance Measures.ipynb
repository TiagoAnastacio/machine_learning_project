{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **1. Performance Measures**\n",
    "\n",
    "Getting a model to work is not enough, we also need to know **how well** it works, especially in scenarios where our model is intended to be deployed in a real-world scenario. Thus, evaluating the performance of a predictive model is as important as building them. Intuitively, a model working implies that the preditions are **close** to the **target values**. However evaluating  how do we measure this closeness?\n",
    "\n",
    "That is the purpose of **performance measures**. These are metrics that quantify the performance of a model, allowing us to compare different models and select the best one for our specific problem. Thus, in this class we will look at the most common performance metrics for both **regression** and **classification** problems. \n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "## Table of Contents\n",
    "### [1 - Regression Problems](#regression)\n",
    "* [1.1. - $R^{2}$ Score](#rsquare)\n",
    "* [1.2. - Adjusted $R^{2}$ Score](#adjusted)\n",
    "* [1.3. - MAE](#mae)\n",
    "* [1.4. - MSE and RMSE](#mse)\n",
    "* [1.5. - MedAE](#medae)\n",
    "* [1.6. - MAPE](#mape)\n",
    "* [1.7. - Comparing Regression Metrics](#comparison)\n",
    "### [2 - Classification Problems](#classification)\n",
    "* [2.1. - The Confusion Matrix](#confusion)\n",
    "* [2.2. - The Accuracy Score](#accuracy)\n",
    "* [2.3. - The Precision](#precision)\n",
    "* [2.4. - The Recall](#recall)\n",
    "* [2.5. - The F1 Score](#f1)\n",
    "* [2.6. - ROC Curve and AUC Score](#roc)\n",
    "* [2.7. - Precision-Recall Curve](#pr-curve)\n",
    "* [2.8. - Comparing Classification Metrics](#classification-comparison)\n",
    "### [3 - Multiclass Classification (Extra)](#multiclass)\n",
    "* [3.1. - Multiclass Confusion Matrix](#multiclass-confusion)\n",
    "* [3.2. - Macro-Averaged Metrics](#macro)\n",
    "* [3.3. - Weighted-Averaged Metrics](#weighted)\n",
    "* [3.4. - Micro-Averaged Metrics](#micro)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"regression\">\n",
    "\n",
    "## 1. Regression Problems\n",
    "</a>\n",
    "\n",
    "For regression problems, the target variable is **continuous**. Underr this cirscumstance, the metrics we generally compute are either based on **proportion of variance explained** or on the **distance between the predicted and actual values**.\n",
    "\n",
    "To see how to compute these metrics, we first will need to create a regression model and obtain predictions... so let's start by doing that.\n",
    "\n",
    "__`Step 0`__ Import the needed libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "#sklearn libraries\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "np.random.seed(33) #for reproducibility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__`Step 1`__ Import the dataset __Boston.csv__ stored in folder `Datasets` and define the independent variables as **data_boston** and call **target_boston** to the dependent variable (last column). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boston = pd.read_csv(r'./Datasets/Boston.csv')\n",
    "data_boston = boston.iloc[:,:-1]\n",
    "target_boston = boston.iloc[:,-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__`Step 2`__ Use the method **train_test_split** from sklearn.model_selection to split your dataset between train (80%) and validation (20%)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(data_boston, \n",
    "                                                    target_boston, \n",
    "                                                    test_size=0.2, \n",
    "                                                    random_state=15, \n",
    "                                                    shuffle=True, \n",
    "                                                   )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__`Step 3`__ Create an instance of LinearRegression named as lr with the default parameters and fit it to your training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression().fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__`Step 4`__ Now that you have your model created, use the method create to assign the predictions to `y_pred_train` and `y_pred_val`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_train = lr.predict(X_train)\n",
    "y_pred_val = lr.predict(X_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__`Step 5`__ From __slearn.metrics__ import r2_score, mean_absolute_error, mean_squared_error, median_absolute_error, and mean_absolute_percentage_error.\n",
    "\n",
    "**Note**: In this notebook we are importing libraries along the way, but in practice it is better to import all the needed libraries at the begining of the notebook/script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error, median_absolute_error, root_mean_squared_error, mean_absolute_percentage_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"rsquare\">\n",
    "    \n",
    "### 1.1. $R^{2}$ Score\n",
    "\n",
    "</a>\n",
    "\n",
    "$R^{2}$, also known as the coefficient of determination, is a statistical measure that represents the proportion of the variance for a dependent variable that's explained by an independent variable or variables in a regression model. It provides an indication of how well the data points fit a statistical model â€“ the **higher the $R^{2}$**, the better the model fits your data, usually ranging from 0 to 1 (though it can be negative if the model is worse than a simple mean predictor). \n",
    "\n",
    "$$\n",
    "R^{2} = 1 - \\frac{SS_{res}}{SS_{tot}}\n",
    "$$\n",
    "Where:\n",
    "- $SS_{res}$ is the sum of squares of residuals (the differences between the observed and predicted values: $\\sum (y_i - \\hat{y}_i)^2$\n",
    "- $SS_{tot}$ is the total sum of squares (the differences between the observed values and the mean of the observed values): $\\sum (y_i - \\bar{y})^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<a href = 'https://scikit-learn.org/stable/modules/generated/sklearn.metrics.r2_score.html#sklearn.metrics.r2_score'>sklearn.metrics.r2_score(y_true, y_pred, ... )</a>\n",
    "\n",
    "__Definition:__ <br>\n",
    "$R^2$ (coefficient of determination) regression score function.\n",
    "\n",
    "__Interpretation:__ <br>\n",
    "Best possible score is 1.0 and it can be negative (because the model can be arbitrarily worse). \n",
    "\n",
    "__Parameters:__ <br>\n",
    "_y_true_: Ground truth (correct) target values; <br>\n",
    "_y_pred_: Estimated target values; <br>\n",
    "...\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__`Step 6`__ Check the $R^2$ score of the model `lr` for both the training and validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_t = r2_score(y_train, y_pred_train)\n",
    "r2_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_v = r2_score(y_val, y_pred_val)\n",
    "r2_v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Advantages of $R^2$**:\n",
    "- Easy to interpret: It ranges between 0 and 1. A value closer to 1 means the model explains a larger proportion of variance.\n",
    "- Common benchmark: It is widely known and used across various fields for evaluating regression models.\n",
    "\n",
    "**Disadvantages of $R^2$**:\n",
    "- Always increases with more predictors: Adding more independent variables to the model will always increase (or at least not decrease) the $R^2$, even if those variables are not statistically significant, which makes it a poor metric to, for example, compare models with different feature sets.\n",
    "- Does not indicate causation: A high $R^2$ does not imply that changes in the independent variables cause changes in the dependent variable.\n",
    "- Does not guarantee good predictions: A model with a high $R^2$ may still have poor predictive performance due to e.g. systematic bias (such as having a model that consistently over or under predicts).\n",
    "- Misleading for non-linear relationships: $R^2$ is based on linear regression assumptions and may not accurately reflect the fit of non-linear models.\n",
    "\n",
    "__When should $R^2$ be used?__ <br>\n",
    "- When we want to measure the amount of variance in the target variable that can be explained by our model. <br>\n",
    "- Comparing models with the same number of predictors (to avoid the pitfall of always increasing $R^2$ with more variables) on a stable, well-defined dataset. <br>\n",
    "- When we want to communicate the explanatory power of a regression model, starting with a familiar metric such as $R^2$ can be useful. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"adjusted\">\n",
    "    \n",
    "### 1.2. Adjusted $R^{2}$ Score ($\\bar{R}^2$)\n",
    "\n",
    "</a>\n",
    "\n",
    "A limitation of $R^2$ is that it always increases (or at least stays the same) when new predictors are added, even if those predictors do not improve the model substantially.<br>\n",
    "To address this issue, **Adjusted R-Squared** modifies the $R^2$ value by taking into account the number of predictors relative to the sample size.<br>\n",
    "\n",
    "If a new predictor improves the model beyond what would be expected by chance, $\\bar{R}^2$ increases.<br>\n",
    "If the predictor does not provide meaningful explanatory power, $\\bar{R}^2$ decreases.<br>\n",
    "Thus, if evaluating explained variance is goal for your project, you should consider $\\bar{R}^2$ as the go-to metric when comparing regression models with different numbers of features.\n",
    "Scikit-Learn does not have a direct implementation of $\\bar{R}^2$ . However, we can compute it using the following formula:\n",
    "\n",
    "$$\n",
    "\\bar{R}^2 = 1 - \\left(1 - R^2\\right) \\cdot \\frac{n - 1}{n - p - 1}\n",
    "$$\n",
    "Where:\n",
    "\n",
    "$\\bar{R}^2$: Adjusted R-Squared<br>\n",
    "$R^2$: R-Squared<br>\n",
    "$n$: number of observations (sample size)<br>\n",
    "$p$: number of predictors (independent variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__`Step 7`__ Calculate the Adjusted R^2 Score for your model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2 = r2_score(y_train, y_pred_train)\n",
    "n = len(y_train)\n",
    "p = len(X_train.columns)\n",
    "\n",
    "def adj_r2 (r2,n,p):\n",
    "    return 1-(1-r2)*(n-1)/(n-p-1)\n",
    "\n",
    "ar2_t = adj_r2(r2,n,p)\n",
    "ar2_t "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO IT\n",
    "r2 = r2_score(y_val, y_pred_val)\n",
    "n = len(y_val)\n",
    "p = len(X_val.columns)\n",
    "\n",
    "ar2_v = adj_r2(r2,n,p)\n",
    "ar2_v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Advantages of $\\bar{R}^2$**:\n",
    "- Penalizes unnecessary predictors: Unlike $R^2$, which can be artificially inflated by adding more variables, $\\bar{R}^2$ decreases when non-significant predictors are added, helping to prevent overfitting.\n",
    "- Fairer for comparison between models with different numbers of predictors.\n",
    "- Retains interpretability: Like $R^2$, it provides insight into the proportion of variance explained by the model, even if computed using a slightly different formula.\n",
    "- Signals risje of overfitting: A significant drop in $\\bar{R}^2$ when adding predictors can indicate that the model is adding *noise* rather than insight.\n",
    "\n",
    "**Disadvantages of $\\bar{R}^2$**:\n",
    "- More complex to compute: Requires knowledge of the number of predictors and sample size, making it less straightforward than $R^2$.\n",
    "- Linear model bias: It is primarily designed for linear regression models and may not be as informative for non-linear models or other non-parametric machine learning algorithms.\n",
    "- Sensitivity to sample size: $\\bar{R}^2$ can be overly sensitive to the number of predictors: may over-estimate the importance of weak predictors in larger datasets and under-estimate them in smaller datasets.\n",
    "- Still is variance-based: Like $R^2$, it does not provide information about the actual prediction error in the units of the target variable.\n",
    "\n",
    "__When should $R^2$ be used?__ <br>\n",
    "- When we are in the process of selecting features for our regression model and want to avoid overfitting by adding too many predictors. Measuring $\\bar{R}^2$ can help identify the point at which adding more features does not significantly improve the model.\n",
    "- When comparing regression models that have different numbers of independent variables, $\\bar{R}^2$ provides a more balanced metric than $R^2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"mae\">\n",
    "    \n",
    "### 1.3. MAE (Mean absolute error)\n",
    "\n",
    "</a>\n",
    "\n",
    "As discussed, the previous metrics are useful to measure the degree of **explainability of a model**, they often fall short when it comes to measuring **how distant a prediction is from the observed value**. So, going forward, we will focus on metrics on **measuring error**. In the literature, you will find multiple different ways of doing this computation. The first example we will cover is likely the most intuitive of them all which is the **Mean Absolute Error (MAE**).\n",
    "\n",
    "$$\n",
    "MAE = \\frac{1}{n} \\sum_{i=1}^{n} |y_i - \\hat{y}_i|\n",
    "$$\n",
    "\n",
    "__`Step 8`__ Check the MAE of the model you created previously"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<a href = 'https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_absolute_error.html#sklearn.metrics.mean_absolute_error'>sklearn.metrics.mean_absolute_error(y_true, y_pred, ... )</a>\n",
    "\n",
    "__Definition:__ <br>\n",
    "Mean absolute error regression loss.\n",
    "\n",
    "__Interpretation:__ <br>\n",
    "Best possible value is 0.0. MAE is always non-negative.\n",
    "\n",
    "__Parameters:__ <br>\n",
    "_y_true_: Ground truth (correct) target values; <br>\n",
    "_y_pred_: Estimated target values; <br>\n",
    "...\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mae_t = mean_absolute_error(y_train, y_pred_train)\n",
    "mae_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mae_v = mean_absolute_error(y_val, y_pred_val)\n",
    "mae_v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Advantages of MAE**\n",
    "- Interpretability: The score computed with MAE is an absolute difference, meaning that it flat out tells you by how much, on average, is the model missing.\n",
    "- Simplicity: Easy to compute and does not involve any computationally challlenging operations (Squares or Roots).\n",
    "- Measures Errors in an Uniform Manner: Useful in problems where the magnitude of the error is not particularly relevant (although this particular point  is a disadvantage when magnitude is relevant).\n",
    "\n",
    "**Diadantages of MAE**\n",
    "- Measures Errors in an Uniform Manner: Essentially, all units of error are considered equally. Consider 2 hypothetical models that could have the same MAE:\n",
    "    1. Makes a very small mistake on every observation,\n",
    "    2. Gets some predictions just right but there a select few that miss by a LOT.\n",
    "- Scale-Dependent: Since its main advantage is interpretability, it loses it if the target is normalized in some manner. \n",
    "\n",
    "__When should MAE be used?__ <br>\n",
    "When it is relevant to communicaate easily with stakeholders: absolute error is very intuitive to understand.<br>\n",
    "When the magnitude of an error is not particularly relevant to the problem (when there is not a disproportionally increased cost of making larger errors)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"mse\">\n",
    "    \n",
    "### 1.4. (Mean Squared Error) and RMSE (Root Mean squared error) \n",
    "</a>\n",
    "\n",
    "These metrics look to address one of the main drawbacks of MAE which is how it addresses the **magnitude of errors**. Under MAE all errors are considered equally important. However, in more critical scenarios, it may be worthwhile to disproportionally **penalize** larger mistakes relative to **smaller mistakes**. MSE and its root form RMSE aim to tackle this issue by **squaring** the difference between target and prediction:\n",
    "\n",
    "$$\n",
    "MSE = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2\n",
    "$$\n",
    "\n",
    "And the Root form:\n",
    "\n",
    "$$\n",
    "RMSE = \\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2} = \\sqrt{MSE}\n",
    "$$\n",
    "\n",
    "__`Step 9`__ Check the RMSE of the model you created previously:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MSE\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<a href = 'https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_squared_error.html#sklearn.metrics.mean_squared_error'>sklearn.metrics.root_mean_squared_error(y_true, y_pred, ... )</a>\n",
    "\n",
    "__Definition:__ <br>\n",
    "Mean squared error regression loss.\n",
    "\n",
    "__Interpretation:__ <br>\n",
    "Best possible value is 0.0. MSE is always non-negative.\n",
    "\n",
    "__Parameters:__ <br>\n",
    "_y_true_: Ground truth (correct) target values; <br>\n",
    "_y_pred_: Estimated target values; <br>\n",
    "...\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_t = mean_squared_error(y_train, y_pred_train)\n",
    "mse_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_v = mean_squared_error(y_val, y_pred_val)\n",
    "mse_v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RMSE\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<a href = 'https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_squared_error.html#sklearn.metrics.root_mean_squared_error'>sklearn.metrics.root_mean_squared_error(y_true, y_pred, ... )</a>\n",
    "\n",
    "__Definition:__ <br>\n",
    "Root mean squared error regression loss.\n",
    "\n",
    "__Interpretation:__ <br>\n",
    "Best possible value is 0.0. RMSE is always non-negative.\n",
    "\n",
    "__Parameters:__ <br>\n",
    "_y_true_: Ground truth (correct) target values; <br>\n",
    "_y_pred_: Estimated target values; <br>\n",
    "...\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse_t = root_mean_squared_error(y_train, y_pred_train)\n",
    "rmse_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse_v = root_mean_squared_error(y_val, y_pred_val)\n",
    "rmse_v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Advantages of MSE and RMSE**:\n",
    "- Stronger penalization of larger errors\n",
    "- Mathematical convenience: unlike MAE, MSE is differentiable at the root, making a very common form of optimization (e.g. OLS in Linear Regression)\n",
    "\n",
    "**Disadvantages of MSE and RMSE**\n",
    "- Oversensitive to outliers: the model is punished for making a prediction that is farther away from the target. If, for some reason, your model does one (or few) predictions that are way off but, other than that, performs decently, the RMSE and to a greater extent MSE scores will likely look worse than it actually is.\n",
    "- Lack of interpretability: MSE works in Square Units which is not very interpretable to most audiences. RMSE tries to address this, but its interpretability is still a long way from MAE.\n",
    "\n",
    "\n",
    "__When should we use MSE?__ <br>\n",
    "MSE is a useful metric to monitor (and optimize for) during model optimization (e.g. hyperparameter tuning) because most **loss functions** usually use Squared Differences.\n",
    "When penalizing larger mistakes is critical and error interpretability is not particularly relevant. MSE penalizations for larger errors are demonstrably larger than MAE and RMSE. \n",
    "\n",
    "__MSE vs. RMSE__ <br>\n",
    "If your goal is merely to select which you should use to select a \"best model\" and that is all you care about you can use one or the other since, results-wise, the ranking of algorithms will be same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"medae\">\n",
    "    \n",
    "### 1.5. MedAE (Median absolute error)\n",
    "\n",
    "</a>\n",
    "\n",
    "While MAE provides the **mean** of absolute errors, there are situations where the **median** might be more informative. The Median Absolute Error (MedAE) calculates the median of the absolute differences between predicted and actual values, making it more **robust to the presence of outlier errors** than MAE.\n",
    "\n",
    "This is particularly useful when your error distribution is **skewed** or contains **extreme values** that might distort the mean. MedAE gives you a better sense of the \"typical\" error your model makes, without being influenced by a few very large mistakes.\n",
    "\n",
    "$$\n",
    "\n",
    "MedAE = median(|y_1 - \\hat{y}_1|, |y_2 - \\hat{y}_2|, ..., |y_n - \\hat{y}_n|)\n",
    "\n",
    "$$\n",
    "\n",
    "__`Step 10`__ Check the MedAE score of the model you created previously\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<a href = 'https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_absolute_error.html#sklearn.metrics.median_absolute_error'>sklearn.metrics.median_absolute_error(y_true, y_pred, ... )</a>\n",
    "\n",
    "__Definition:__ <br>\n",
    "Median absolute error regression loss\n",
    "\n",
    "__Interpretation:__ <br>\n",
    "Best possible value is 0.0. MedAE is always non-negative.\n",
    "\n",
    "__Parameters:__ <br>\n",
    "_y_true_: Ground truth (correct) target values; <br>\n",
    "_y_pred_: Estimated target values; <br>\n",
    "...\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "medae_t = median_absolute_error(y_train, y_pred_train)\n",
    "medae_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "medae_v =median_absolute_error(y_val, y_pred_val)\n",
    "medae_v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Advantages of MedAE**:\n",
    "- Robust to outliers: Unlike MAE, which can be influenced by extreme values, MedAE uses the median, making it less sensitive to outliers in the error distribution.\n",
    "- Intuitive interpretation: Like MAE, MedAE provides a direct measure of error magnitude in the same units as the target variable.\n",
    "- Better representation of typical error: In datasets with skewed error distributions, MedAE can provide a better sense of the *typical* error than MAE.\n",
    "\n",
    "**Disadvantages of MedAE**:\n",
    "- Less sensitive to all errors: While robustness to outliers can be an advantage, it also means that MedAE might not adequately reflect the presence of significant errors in the model.\n",
    "- Scale-dependent: Like MAE, MedAE loses interpretability when the target variable is normalized or scaled.\n",
    "- Less commonly used: MedAE is not as widely adopted as other metrics, which can make comparison with other studies or benchmarks more difficult.\n",
    "\n",
    "__When should MedAE be used?__ <br>\n",
    "- When your dataset contains outliers or extreme values that you want to de-emphasize in the evaluation.\n",
    "- When you want to understand the typical error magnitude without being influenced by a few very large errors.\n",
    "- When the error distribution is highly skewed and you want a more representative measure of central tendency in the errors.\n",
    "- In combination with other metrics to get a more complete picture of model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"mape\">\n",
    "    \n",
    "### 1.6. MAPE (Mean absolute percentage error)\n",
    "\n",
    "</a>\n",
    "\n",
    "All the metrics we've seen so far (MAE, MSE, RMSE, MedAE) are **scale-dependent**, meaning their values depend on the units and scale of your target variable. This makes it difficult to compare model performance across different datasets or when the target variable has very different ranges.\n",
    "\n",
    "The Mean Absolute Percentage Error (MAPE) addresses this limitation by expressing errors as **percentages** relative to the actual values. This makes MAPE **scale-independent** and highly interpretable - a MAPE of 10% means your model is off by 10% on average, regardless of whether you're predicting house prices in thousands or stock returns in decimals.\n",
    "\n",
    "MAPE is particularly valuable in **business contexts** where stakeholders prefer to understand model performance in terms of percentage errors rather than absolute values.\n",
    "\n",
    "$$\n",
    "\n",
    "MAPE = \\frac{100\\%}{n} \\sum_{i=1}^{n} \\left|\\frac{y_i - \\hat{y}_i}{y_i}\\right|\n",
    "\n",
    "$$\n",
    "\n",
    "__`Step 11`__ Check the MAPE score of the model you created previously"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<a href = 'https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_absolute_percentage_error.html#sklearn.metrics.mean_absolute_percentage_error'>sklearn.metrics.mean_absolute_percentage_error(y_true, y_pred, ... )</a>\n",
    "\n",
    "__Definition:__ <br>\n",
    "Mean absolute percentage error (MAPE) regression loss.\n",
    "\n",
    "__Interpretation:__ <br>\n",
    "Best possible value is 0.0. MAPE is always non-negative and expressed as a percentage.\n",
    "\n",
    "__Parameters:__ <br>\n",
    "_y_true_: Ground truth (correct) target values; <br>\n",
    "_y_pred_: Estimated target values; <br>\n",
    "...\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mape_t = mean_absolute_percentage_error(y_train, y_pred_train)\n",
    "mape_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mape_v = mean_absolute_percentage_error(y_val, y_pred_val)\n",
    "mape_v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Advantages of MAPE**:\n",
    "- Scale-independent: Unlike MAE, MSE, and RMSE, MAPE is expressed as a percentage, making it easy to interpret and compare across different datasets with different scales.\n",
    "- Intuitive interpretation: The result is directly interpretable as \"the model is off by X% on average.\"\n",
    "- Useful for business contexts: Stakeholders often find percentage errors more meaningful than absolute errors.\n",
    "\n",
    "**Disadvantages of MAPE**:\n",
    "- Division by zero issues: MAPE cannot be calculated when any actual values are zero, as this would result in division by zero.\n",
    "- Asymmetric penalty: MAPE penalizes negative errors (under-predictions) more than positive errors (over-predictions) of the same magnitude.\n",
    "- Sensitive to small actual values: When actual values are close to zero, even small absolute errors can result in very large percentage errors.\n",
    "- Not suitable for data with negative values: MAPE is undefined for negative actual values.\n",
    "\n",
    "__When should MAPE be used?__ <br>\n",
    "- When you need a scale-independent metric that can be easily communicated to stakeholders.\n",
    "- When comparing models across different datasets or when the target variable spans different orders of magnitude.\n",
    "- In business contexts where percentage errors are more meaningful than absolute errors (e.g., sales forecasting, inventory management).\n",
    "- When the actual values are consistently positive and not close to zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing Differences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a dataframe with all the metrics calculated\n",
    "regression_metrics = pd.DataFrame({\n",
    "    'Metric': ['R2', 'Adjusted R2', 'MAE', 'MSE', 'RMSE', 'MedAE', 'MAPE'],\n",
    "    'Train': [r2_t, ar2_t, mae_t, mse_t, rmse_t, medae_t, mape_t],\n",
    "    'Validation': [r2_v, ar2_v, mae_v, mse_v, rmse_v, medae_v, mape_v],\n",
    "    })\n",
    "\n",
    "regression_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"comparison\">\n",
    "\n",
    "### 1.7. How Metrics Can Change Model Selection: A Numerical Example\n",
    "\n",
    "</a>\n",
    "\n",
    "So far, we have seen a set of different metrics, each with their own pros and cons. The natural question that follows is: *which one should I choose?*\n",
    "\n",
    "The answer, as it often is in machine learning, is: *it depends*. It depends on your data, your business context, and what you consider a \"good\" prediction.\n",
    "\n",
    "To illustrate this, let's create a small, controlled example with a few hypothetical models. We will see how the \"best\" model can change depending on the metric we use to evaluate it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__`Step 12`__ Let's define a set of true values and predictions from three hypothetical models.\n",
    "\n",
    "To make this concrete, imagine we are predicting house prices (in thousands of dollars).\n",
    "\n",
    "*   **`y_true`**: The actual prices of 6 houses.\n",
    "*   **`preds_model_A`**: A model that is off by a small, consistent amount for each prediction.\n",
    "*   **`preds_model_B`**: A model that gets most predictions almost perfect, but makes one very large error.\n",
    "*   **`preds_model_C`**: A model that has a mix of perfect predictions and some larger errors, but none as extreme as Model B."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Actual values\n",
    "y_true = np.array([100, 150, 200, 250, 300, 350])\n",
    "\n",
    "# Model A: Consistent, small errors\n",
    "preds_model_A = np.array([110, 160, 210, 260, 310, 360])\n",
    "\n",
    "# Model B: Mostly perfect, one large error\n",
    "preds_model_B = np.array([100, 150, 200, 250, 300, 450])\n",
    "\n",
    "# Model C: A mix of perfect and moderate errors\n",
    "preds_model_C = np.array([100, 150, 225, 275, 300, 350])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__`Step 13`__ First, create a helper function to calculate all our regression metrics for a given set of predictions. This will make our code cleaner and easier to read."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_regression_metrics(y_true, y_pred):\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    rmse = root_mean_squared_error(y_true, y_pred)\n",
    "    medae = median_absolute_error(y_true, y_pred)\n",
    "    mape = mean_absolute_percentage_error(y_true, y_pred)\n",
    "    \n",
    "    return [r2, mae, mse, rmse, medae, mape]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__`Step 14`__ Use our function to compute the metrics for each of the three models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_A = calculate_regression_metrics(y_true, preds_model_A)\n",
    "metrics_B = calculate_regression_metrics(y_true, preds_model_B)\n",
    "metrics_C = calculate_regression_metrics(y_true, preds_model_C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__`Step 15`__ Finally, organize these results into a DataFrame to make them easy to compare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison_df = pd.DataFrame({\n",
    "    'Metric': ['R2', 'MAE', 'MSE', 'RMSE', 'MedAE', 'MAPE'],\n",
    "    'Model A': metrics_A,\n",
    "    'Model B': metrics_B,\n",
    "    'Model C': metrics_C\n",
    "})\n",
    "\n",
    "comparison_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__`Step 16`__ Analyze the results and see what they tell us.\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "### Analysis of the Results\n",
    "\n",
    "Looking at the table above, we can see that the \"best\" model changes depending on the metric used:\n",
    "\n",
    "* **Model A (Consistent and Small Errors) is the best** according to $R^2$ (0.98), $MSE$ (100) and $RMSE$ (10).\n",
    "* **Model B (Mostly Perfect with One Large Error) is the best** according to $MedAE$ (tied with C at 0) and by far the worst model when it comes to $MSE$ (1666.7) and $RMSE$ (40.8).\n",
    "* **Model C (A less extreme version of Model B)** is the best on $MAPE$ (0.0375), $MAE$ (8.33) and $MedAE$ (tied with Model B at 0)\n",
    "\n",
    "### Key Takeaway\n",
    "\n",
    "The choice of appropriate metric(s) is a crucial aspect of Supervised Learning. Your decision should be based on the business problem.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"classification\">\n",
    "\n",
    "## 2. Classification Problems\n",
    "</a>\n",
    "\n",
    "In ckassification problems, the target variable is often a **nominal categorical**. Thus, when predicting, the output will either match the target variable or not. This means that the metrics we will use to evaluate classification models will mostly be based on **counting** how many predictions were correct and how many were incorrect. Similarly to the regression case, we will first need to create a classification model and obtain predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__`Step 17`__ Import the needed libraries to apply Logistic Regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__`Step 18`__ Import the dataset __final_tugas.csv__ and define the independent variables as __data__ and the dependent variable ('DepVar') as __target__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tugas = pd.read_csv(r'./Datasets/final_tugas.csv')\n",
    "data_tugas = tugas.drop(['DepVar'], axis=1)\n",
    "target_tugas = tugas['DepVar']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__`Step 19`__ Use `train_test_split` from `sklearn.model_selection` to split your dataset into train (80%) and validation (20%)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(data_tugas, \n",
    "                                                  target_tugas, \n",
    "                                                  test_size = 0.2, \n",
    "                                                  random_state=5, \n",
    "                                                  stratify = target_tugas) #in this case, we use stratify to keep the same proportion of classes in train and validation sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__`Step 20`__ Create an instance of `LogisticRegression` named `log_model` with the default parameters and fit it to your training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_model = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__`Step 21`__ Once the model is trained, obtain predictions for both the training and validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_train = log_model.predict(X_train)\n",
    "y_pred_val = log_model.predict(X_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__`Step 22`__ From __sklearn.metrics__ import `confusion_matrix`, `accuracy_score`, `precision_score`, `recall_score`, and `f1_score`, `auc_score`, `precision_recall_curve`, `roc_curve`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The metrics used for classification differ from the ones used for regression. For classification problems, the target variable is often a **nominal categorical**. Thus, when predicting, the output will either match the target variable or not. This means that the metrics we will use to evaluate classification models will mostly be based on **counting** how many predictions were correct and how many were incorrect. \n",
    "\n",
    "Most classification metrics are calculated having, as basis, the **confusion matrix**. This matrix summarizes how the predictions of a classification model compare to the actual values, and serves as the foundation for computing accuracy, precision, recall, and other performance measures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, precision_recall_curve, roc_curve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"confusion\">\n",
    "    \n",
    "### 2.1. The confusion matrix\n",
    "\n",
    "</a>\n",
    "\n",
    "The confusion matrix is a table that is often used to describe the performance of a classification model (or \"classifier\") by counting the number of correct and incorrect predictions made by the model compared to the actual outcomes (target values) in the data. Its shape is typically a square matrix, where the rows represent the actual classes and the columns represent the predicted classes. In a binary classification problem, the confusion matrix is a 2x2 table with the following components:\n",
    "\n",
    "|                      | Predicted Negative | Predicted Positive |\n",
    "|----------------------|--------------------|--------------------|\n",
    "| **Actual Negative**   | True Negative (TN) | False Positive (FP)|\n",
    "| **Actual Positive**   | False Negative (FN)| True Positive (TP) |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<a href = 'https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html#sklearn.metrics.confusion_matrix'>sklearn.metrics.confusion_matrix(y_true, y_pred, ...)</a>\n",
    "\n",
    "__Definition:__ <br>\n",
    "Compute confusion matrix to evaluate the accuracy of a classification\n",
    "\n",
    "__Parameters:__ <br>\n",
    "_y_true_: Ground truth (correct) target values.; <br>\n",
    "_y_pred_: Estimated targets as returned by a classifier.; <br>\n",
    "...\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__`Step 23`__ Obtain the confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_train = confusion_matrix(y_train, y_pred_train)\n",
    "cm_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_val = confusion_matrix(y_val, y_pred_val)\n",
    "cm_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Understanding the Confusion Matrix**\n",
    "\n",
    "The confusion matrix in sklearn is presented in the following format:\n",
    "\n",
    "```\n",
    "[[TN  FP]\n",
    " [FN  TP]]\n",
    "```\n",
    "\n",
    "Where:\n",
    "- **TN (True Negatives)**: Cases correctly predicted as negative\n",
    "- **FP (False Positives)**: Cases incorrectly predicted as positive (Type I Error)\n",
    "- **FN (False Negatives)**: Cases incorrectly predicted as negative (Type II Error)\n",
    "- **TP (True Positives)**: Cases correctly predicted as positive\n",
    "\n",
    "**Advantages of Confusion Matrix**:\n",
    "- Comprehensive view: Shows all possible outcomes of predictions (TP, TN, FP, FN).\n",
    "- Foundation for other metrics: Most classification metrics are derived from the confusion matrix.\n",
    "- Error analysis: Helps identify which type of errors (false positives vs false negatives) the model is making.\n",
    "- Visual interpretation: Easy to visualize and understand model behavior.\n",
    "\n",
    "**Disadvantages of Confusion Matrix**:\n",
    "- Not a single metric: Cannot directly compare models with a single number.\n",
    "- Requires interpretation: Need to understand what each cell means in the context of your problem.\n",
    "- Scale dependent: Raw counts can be misleading with imbalanced datasets.\n",
    "\n",
    "**When should the Confusion Matrix be used?**\n",
    "- When you need a detailed breakdown of model predictions.\n",
    "- When understanding the types of errors is as important as the overall accuracy.\n",
    "- As a starting point for computing other classification metrics.\n",
    "- When communicating model performance to stakeholders who need to understand specific error types."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"accuracy\">\n",
    "    \n",
    "### 2.2. The Accuracy Score\n",
    "\n",
    "</a>\n",
    "\n",
    "**Accuracy** is the most intuitive classification metric. It measures the proportion of correct predictions (both positive and negative) out of all predictions made. While simple to understand, accuracy can be misleading when dealing with imbalanced datasets.\n",
    "\n",
    "$$\n",
    "Accuracy = \\frac{TP + TN}{TP + TN + FP + FN}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- **TP**: True Positives\n",
    "- **TN**: True Negatives\n",
    "- **FP**: False Positives\n",
    "- **FN**: False Negatives\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<a href = 'https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html#sklearn.metrics.accuracy_score'>sklearn.metrics.accuracy_score(y_true, y_pred, normalize=True,...)</a>\n",
    "\n",
    "__Definition:__ <br>\n",
    "Accuracy classification score.\n",
    "\n",
    "__Interpretation:__ <br>\n",
    "If normalize is True, then the best performance is 1. When normalize = False, then the best performance is the number of samples.\n",
    "\n",
    "__Parameters:__ <br>\n",
    "_y_true_: Ground truth (correct) target values.; <br>\n",
    "_y_pred_: Estimated targets as returned by a classifier.; <br>\n",
    "_normalize_: If False, return the number of correctly classified samples. Otherwise, return the fraction of correctly classified samples. <br>\n",
    "...\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__`Step 24`__ Compute the accuracy score for both the training and validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_train = accuracy_score(y_train, y_pred_train)\n",
    "accuracy_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_val = accuracy_score(y_val, y_pred_val)\n",
    "accuracy_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Advantages of Accuracy**:\n",
    "- Simple and intuitive: Easy to understand and explain to non-technical stakeholders.\n",
    "- Single metric: Provides one number to compare different models.\n",
    "- Balanced measure: Considers both positive and negative predictions.\n",
    "- Widely used: Common benchmark across many classification problems.\n",
    "\n",
    "**Disadvantages of Accuracy**:\n",
    "- Misleading with imbalanced data: Can show high values even when the model performs poorly on the minority class.\n",
    "- Treats all errors equally: Doesn't distinguish between false positives and false negatives, which may have different costs.\n",
    "- Not informative about error types: Doesn't tell you which class the model struggles with.\n",
    "\n",
    "**Is accuracy always a good option?**\n",
    "\n",
    "Let's check with an example:\n",
    "\n",
    "<img src=\"images/example_1.png\" alt=\"Drawing\" style=\"width: 400px;\"/>\n",
    "\n",
    "In this case, what is the accuracy?\n",
    "\n",
    "<img src=\"images/example_2.png\" alt=\"Drawing\" style=\"width: 300px;\"/>\n",
    "\n",
    "We have an accuracy of 99.1% which is very very high! That is great, right?\n",
    "\n",
    "**Well, not really...**\n",
    "\n",
    "Imagine that we are testing people potentially with COVID... A positive person is actually someone who is sick and carrying a virus that can spread very quickly! The cost of having a misclassified actual positive (or a false negative) is very high! In this scenario, we would need to look at other metrics that focus on capturing all positive cases.\n",
    "\n",
    "**When should Accuracy be used?**\n",
    "- When the dataset is balanced (roughly equal number of samples in each class).\n",
    "- When false positives and false negatives have similar costs.\n",
    "- When you need a simple, interpretable metric for initial model evaluation.\n",
    "- When all classes are equally important to predict correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"precision\">\n",
    "    \n",
    "### 2.3. The Precision\n",
    "\n",
    "</a>\n",
    "\n",
    "**Precision** measures the accuracy of positive predictions. It answers the question: \"Of all the instances we predicted as positive, how many were actually positive?\" This metric is particularly important when the cost of false positives is high.\n",
    "\n",
    "$$\n",
    "Precision = \\frac{TP}{TP + FP}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- **TP**: True Positives (correctly predicted positive cases)\n",
    "- **FP**: False Positives (incorrectly predicted positive cases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<a href = 'https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_score.html#sklearn.metrics.precision_score'>sklearn.metrics.precision_score(y_true, y_pred, ...)</a>\n",
    "\n",
    "__Definition:__ <br>\n",
    "Compute the precision.\n",
    "\n",
    "__Interpretation:__ <br>\n",
    "The best value is 1, and the worst value is 0.\n",
    "\n",
    "__Parameters:__ <br>\n",
    "_y_true_: Ground truth (correct) target values.; <br>\n",
    "_y_pred_: Estimated targets as returned by a classifier.; <br>\n",
    "...\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__`Step 25`__ Compute the precision score for both the training and validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_train = precision_score(y_train, y_pred_train)\n",
    "precision_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_val = precision_score(y_val, y_pred_val)\n",
    "precision_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Understanding Precision**\n",
    "\n",
    "If you look at the confusion matrix, we can verify that precision is only concerned with the predicted values that were considered positive:\n",
    "    \n",
    "<img src=\"images/example_3.png\" alt=\"Drawing\" style=\"width: 400px;\"/>\n",
    "\n",
    "So precision tells us how precise/accurate our model is: out of those predicted positive, how many of them are actually positive.\n",
    "\n",
    "**Advantages of Precision**:\n",
    "- Focus on positive predictions: Directly measures the reliability of positive predictions.\n",
    "- Important for specific applications: Critical when false positives are costly.\n",
    "- Easy to interpret: Straightforward percentage of correct positive predictions.\n",
    "- Complements recall: Together they provide a complete picture of positive class performance.\n",
    "\n",
    "**Disadvantages of Precision**:\n",
    "- Ignores false negatives: Doesn't account for positive cases that were missed.\n",
    "- Can be manipulated: A model that predicts very few positives can have high precision but miss many actual positives.\n",
    "- Class imbalance sensitive: Can be misleading in imbalanced datasets.\n",
    "- Incomplete on its own: Should be used alongside recall for full understanding.\n",
    "\n",
    "**When should Precision be used?**\n",
    "\n",
    "`When the cost of False Positives is high.`\n",
    "\n",
    "**Example**: Email spam detection\n",
    "- Negative = Not spam\n",
    "- Positive = Spam\n",
    "\n",
    "A **false positive** means a legitimate email is classified as spam. If precision is low, important emails (like job offers, client communications, or medical results) might end up in the spam folder, causing the user to miss critical information. High precision ensures that when an email is marked as spam, it really is spam.\n",
    "\n",
    "**Other examples where precision matters**:\n",
    "- Medical diagnosis for expensive treatments (don't want to treat healthy patients)\n",
    "- Fraud detection in banking (don't want to block legitimate transactions)\n",
    "- Content moderation (don't want to wrongly remove appropriate content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"recall\">\n",
    "    \n",
    "### 2.4. The Recall\n",
    "\n",
    "</a>\n",
    "\n",
    "**Recall** (also known as **Sensitivity** or **True Positive Rate**) measures the ability of a model to find all positive cases. It answers the question: \"Of all the actual positive instances, how many did we correctly identify?\" This metric is crucial when the cost of false negatives is high.\n",
    "\n",
    "$$\n",
    "Recall = \\frac{TP}{TP + FN}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- **TP**: True Positives (correctly predicted positive cases)\n",
    "- **FN**: False Negatives (positive cases incorrectly predicted as negative)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<a href = 'https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_score.html#sklearn.metrics.recall_score'>sklearn.metrics.recall_score(y_true, y_pred, ...)</a>\n",
    "\n",
    "__Definition:__ <br>\n",
    "Compute the recall.\n",
    "\n",
    "__Interpretation:__ <br>\n",
    "The best value is 1 and the worst value is 0.\n",
    "\n",
    "__Parameters:__ <br>\n",
    "_y_true_: Ground truth (correct) target values.; <br>\n",
    "_y_pred_: Estimated targets as returned by a classifier.; <br>\n",
    "...\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__`Step 26`__ Compute the recall score for both the training and validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recall_train = recall_score(y_train, y_pred_train)\n",
    "recall_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recall_val = recall_score(y_val, y_pred_val)\n",
    "recall_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Understanding Recall**\n",
    "\n",
    "Looking at the confusion matrix:\n",
    "    \n",
    "<img src=\"images/example_4.png\" alt=\"Drawing\" style=\"width: 400px;\"/>\n",
    "\n",
    "Recall calculates how many of the actual positives our model is able to capture by labeling them as positive (True Positive). It measures the model's **completeness** in identifying positive cases.\n",
    "\n",
    "**Advantages of Recall**:\n",
    "- Focuses on completeness: Ensures we don't miss positive cases.\n",
    "- Critical for safety: Important when missing a positive case has serious consequences.\n",
    "- Complements precision: Together they provide complete evaluation of positive class performance.\n",
    "- Intuitive interpretation: Directly shows what percentage of actual positives were found.\n",
    "\n",
    "**Disadvantages of Recall**:\n",
    "- Ignores false positives: Doesn't consider how many negatives were incorrectly classified as positive.\n",
    "- Can be manipulated: A model that predicts everything as positive will have perfect recall but be useless.\n",
    "- Incomplete on its own: Must be balanced with precision to avoid excessive false positives.\n",
    "- May lead to over-prediction: Optimizing only for recall can result in too many positive predictions.\n",
    "\n",
    "**When should Recall be used?**\n",
    "\n",
    "`When the cost of False Negatives is high.`\n",
    "\n",
    "**Example**: COVID-19 Testing\n",
    "- Negative = Not sick\n",
    "- Positive = Sick with COVID-19\n",
    "\n",
    "A **false negative** means a sick patient is told they are healthy. This is extremely dangerous because:\n",
    "- The patient won't get treatment they need\n",
    "- They will continue their normal activities and spread the virus to others\n",
    "- The disease could worsen without medical intervention\n",
    "\n",
    "High recall ensures we catch as many actual COVID cases as possible, even if it means some healthy people are initially flagged (false positives can be filtered with additional testing).\n",
    "\n",
    "**Other examples where recall matters**:\n",
    "- Cancer screening (don't want to miss any cancer cases)\n",
    "- Fraud detection in credit cards (don't want to miss fraudulent transactions)\n",
    "- Security threat detection (don't want to miss potential threats)\n",
    "- Disease outbreak detection (need to catch all potential cases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"f1\">\n",
    "    \n",
    "### 2.5. The F1 Score\n",
    "\n",
    "</a>\n",
    "\n",
    "The **F1 Score** is the harmonic mean of precision and recall. It provides a single metric that balances both concerns: finding all positive cases (recall) and ensuring predictions are accurate (precision). The F1 score is particularly useful when you need a balance between precision and recall, or when dealing with imbalanced datasets.\n",
    "\n",
    "$$\n",
    "F1 = 2 \\times \\frac{Precision \\times Recall}{Precision + Recall} = \\frac{2 \\times TP}{2 \\times TP + FP + FN}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- **TP**: True Positives\n",
    "- **FP**: False Positives\n",
    "- **FN**: False Negatives\n",
    "\n",
    "**Why use harmonic mean instead of arithmetic mean?**\n",
    "The harmonic mean penalizes extreme values more than the arithmetic mean. If either precision or recall is very low, the F1 score will also be low, even if the other metric is high."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<a href = 'https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html#sklearn.metrics.f1_score'>sklearn.metrics.f1_score(y_true, y_pred, ...)</a>\n",
    "\n",
    "__Definition:__ <br>\n",
    "Compute the F1 score, also known as balanced F-score or F-measure.\n",
    "\n",
    "__Interpretation:__ <br>\n",
    "F1 score reaches its best value at 1 and worst score at 0.\n",
    "\n",
    "__Parameters:__ <br>\n",
    "_y_true_: Ground truth (correct) target values.; <br>\n",
    "_y_pred_: Estimated targets as returned by a classifier.; <br>\n",
    "...\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__`Step 27`__ Compute the F1 score for both the training and validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_train = f1_score(y_train, y_pred_train)\n",
    "f1_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_val = f1_score(y_val, y_pred_val)\n",
    "f1_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Advantages of F1 Score**:\n",
    "- Balanced metric: Combines both precision and recall into a single score.\n",
    "- Handles imbalance well: More informative than accuracy for imbalanced datasets.\n",
    "- Penalizes extreme cases: The harmonic mean ensures both precision and recall need to be good.\n",
    "- Single metric for optimization: Useful when you need to optimize or compare models with one number.\n",
    "\n",
    "**Disadvantages of F1 Score**:\n",
    "- Equal weighting: Treats precision and recall as equally important, which may not match real-world costs.\n",
    "- Less interpretable: Not as intuitive as precision or recall individually.\n",
    "- Ignores true negatives: Doesn't account for correctly predicted negative cases.\n",
    "- May not reflect business goals: Sometimes precision or recall alone is more aligned with business objectives.\n",
    "\n",
    "**When should F1 Score be used?**\n",
    "- When you need to seek a **balance between precision and recall**.\n",
    "- When there is an **uneven class distribution** (large number of actual negatives).\n",
    "- When **both false positives and false negatives have significant (and similar) costs**.\n",
    "- When you need a **single metric** to compare models but accuracy is misleading due to class imbalance.\n",
    "\n",
    "**Example Use Cases**:\n",
    "- Information retrieval systems (search engines need both precision and recall)\n",
    "- Medical diagnosis where both missing cases and false alarms are problematic\n",
    "- Customer churn prediction (want to identify churners without annoying loyal customers)\n",
    "- Quality control systems (need to catch defects without rejecting good products)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding Classification Thresholds\n",
    "\n",
    "Before we dive into ROC curves and Precision-Recall curves, it's crucial to understand what a **classification threshold** is and why it matters.\n",
    "\n",
    "### What is a Classification Threshold?\n",
    "\n",
    "Most classification models (like Logistic Regression) don't directly output class labels (0 or 1). Instead, they output **probabilities** that represent the model's confidence that an instance belongs to the positive class.\n",
    "\n",
    "For example:\n",
    "- Probability = 0.85 â†’ Model is 85% confident this is a positive case\n",
    "- Probability = 0.23 â†’ Model is 23% confident this is a positive case\n",
    "\n",
    "To convert these probabilities into actual predictions (0 or 1), we need to set a **threshold**:\n",
    "- If probability â‰¥ threshold â†’ Predict **Positive (1)**\n",
    "- If probability < threshold â†’ Predict **Negative (0)**\n",
    "\n",
    "**The default threshold is usually 0.5**, but this is not always optimal!\n",
    "\n",
    "### Why Does the Threshold Matter?\n",
    "\n",
    "Different thresholds lead to different trade-offs between precision and recall, and between true positives and false positives:\n",
    "\n",
    "- **Lower threshold (e.g., 0.3)**: \n",
    "  - More instances classified as positive\n",
    "  - Higher recall (catch more positives)\n",
    "  - Lower precision (more false positives)\n",
    "  - Use when: Missing a positive case is very costly\n",
    "\n",
    "- **Higher threshold (e.g., 0.7)**:\n",
    "  - Fewer instances classified as positive\n",
    "  - Lower recall (miss some positives)\n",
    "  - Higher precision (fewer false positives)\n",
    "  - Use when: False alarms are very costly\n",
    "\n",
    "### Example: Medical Screening\n",
    "\n",
    "Imagine a COVID-19 screening model that outputs probabilities:\n",
    "\n",
    "| Patient | Probability | True Status |\n",
    "|---------|------------|-------------|\n",
    "| A       | 0.95       | Positive    |\n",
    "| B       | 0.65       | Positive    |\n",
    "| C       | 0.45       | Negative    |\n",
    "| D       | 0.30       | Positive    |\n",
    "| E       | 0.10       | Negative    |\n",
    "\n",
    "**With threshold = 0.5:**\n",
    "- Predictions: A=Positive, B=Positive, C=Negative, D=Negative, E=Negative\n",
    "- Missed patient D (False Negative)\n",
    "- Recall = 2/3 = 0.67\n",
    "\n",
    "**With threshold = 0.3:**\n",
    "- Predictions: A=Positive, B=Positive, C=Positive, D=Positive, E=Negative\n",
    "- Caught all sick patients!\n",
    "- Recall = 3/3 = 1.0\n",
    "- But patient C is a false positive\n",
    "\n",
    "The choice depends on your priorities: **Would you rather miss sick patients or test more people?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__`Step 27a`__ Let's demonstrate how different thresholds affect our predictions. First, get the probability predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get probability predictions for the positive class\n",
    "y_pred_proba_val = log_model.predict_proba(X_val)[:, 1]\n",
    "\n",
    "# Show first 10 probabilities\n",
    "print(\"First 10 probability predictions:\")\n",
    "print(y_pred_proba_val[:10])\n",
    "print(\"\\nActual labels:\")\n",
    "print(y_val.values[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__`Step 27b`__ Now let's compare how different thresholds (0.3, 0.5, and 0.7) affect precision, recall, and F1 score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test different thresholds\n",
    "thresholds_to_test = [0.3, 0.5, 0.7]\n",
    "results = []\n",
    "\n",
    "for threshold in thresholds_to_test:\n",
    "    # Apply threshold to get predictions\n",
    "    y_pred_threshold = (y_pred_proba_val >= threshold).astype(int)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    acc = accuracy_score(y_val, y_pred_threshold)\n",
    "    prec = precision_score(y_val, y_pred_threshold, zero_division=0)\n",
    "    rec = recall_score(y_val, y_pred_threshold, zero_division=0)\n",
    "    f1 = f1_score(y_val, y_pred_threshold, zero_division=0)\n",
    "    \n",
    "    results.append([threshold, acc, prec, rec, f1])\n",
    "\n",
    "# Create comparison table\n",
    "threshold_comparison = pd.DataFrame(results, \n",
    "                                    columns=['Threshold', 'Accuracy', 'Precision', 'Recall', 'F1'])\n",
    "threshold_comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations:**\n",
    "- **Lower threshold (0.3)**: Typically higher recall (catches more positives) but lower precision (more false alarms)\n",
    "- **Default threshold (0.5)**: Balanced trade-off\n",
    "- **Higher threshold (0.7)**: Typically higher precision (fewer false alarms) but lower recall (misses more positives)\n",
    "\n",
    "The question is: **How do we choose the optimal threshold?** That's where ROC curves and Precision-Recall curves come in!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"roc\">\n",
    "    \n",
    "### 2.6. ROC Curve and AUC Score\n",
    "\n",
    "</a>\n",
    "\n",
    "So far, all the metrics we've discussed have been based on **hard predictions** (0 or 1). However, most classification models actually output **probabilities** (e.g., 0.85 means 85% confident the instance is positive). The **ROC (Receiver Operating Characteristic) Curve** and **AUC (Area Under the Curve) Score** are metrics that evaluate model performance across **all possible classification thresholds**.\n",
    "\n",
    "#### What is the ROC Curve?\n",
    "\n",
    "The ROC curve is a graphical plot that illustrates the diagnostic ability of a binary classifier as its discrimination threshold is varied. It plots:\n",
    "- **Y-axis**: True Positive Rate (TPR) = Recall = $\\frac{TP}{TP + FN}$\n",
    "- **X-axis**: False Positive Rate (FPR) = $\\frac{FP}{FP + TN}$\n",
    "\n",
    "By varying the classification threshold from 0 to 1, we get different combinations of TPR and FPR, which create the ROC curve.\n",
    "\n",
    "#### What is the AUC Score?\n",
    "\n",
    "The **AUC (Area Under the ROC Curve)** score measures the entire two-dimensional area underneath the entire ROC curve. It provides an aggregate measure of performance across all possible classification thresholds.\n",
    "\n",
    "**AUC Interpretation:**\n",
    "- **AUC = 1.0**: Perfect classifier (can perfectly separate classes at some threshold)\n",
    "- **AUC = 0.5**: Random classifier (no better than flipping a coin)\n",
    "- **AUC < 0.5**: Worse than random (predictions are systematically wrong)\n",
    "- **AUC between 0.7-0.8**: Acceptable performance\n",
    "- **AUC between 0.8-0.9**: Excellent performance\n",
    "- **AUC > 0.9**: Outstanding performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<a href='https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html'>sklearn.metrics.roc_auc_score(y_true, y_score, ...)</a>\n",
    "\n",
    "__Definition:__ <br>\n",
    "Compute Area Under the Receiver Operating Characteristic Curve (ROC AUC) from prediction scores.\n",
    "\n",
    "__Interpretation:__ <br>\n",
    "Best possible score is 1.0. A score of 0.5 indicates random predictions.\n",
    "\n",
    "__Parameters:__ <br>\n",
    "_y_true_: Ground truth (correct) target values; <br>\n",
    "_y_score_: Target scores (probability estimates of the positive class); <br>\n",
    "...\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__`Step 28`__ First, we need to obtain the **probability scores** instead of hard predictions. Use the `predict_proba` method to get probability estimates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get probability predictions for the positive class (column 1)\n",
    "y_pred_proba_train = log_model.predict_proba(X_train)[:, 1]\n",
    "y_pred_proba_val = log_model.predict_proba(X_val)[:, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__`Step 29`__ Compute the AUC score for both training and validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auc_train = roc_auc_score(y_train, y_pred_proba_train)\n",
    "auc_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auc_val = roc_auc_score(y_val, y_pred_proba_val)\n",
    "auc_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__`Step 30`__ Now let's plot the ROC curve to visualize the trade-off between True Positive Rate and False Positive Rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute ROC curve for validation set\n",
    "fpr, tpr, thresholds = roc_curve(y_val, y_pred_proba_val)\n",
    "\n",
    "# Plot ROC curve\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, color='blue', lw=2, label=f'ROC curve (AUC = {auc_val:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='gray', lw=2, linestyle='--', label='Random Classifier (AUC = 0.50)')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate', fontsize=12)\n",
    "plt.ylabel('True Positive Rate (Recall)', fontsize=12)\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve', fontsize=14)\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.grid(alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Understanding the ROC Curve:**\n",
    "- The **diagonal line** (gray dashed line) represents a random classifier\n",
    "- The **closer the curve is to the top-left corner**, the better the model\n",
    "- A **perfect classifier** would have a point at (0, 1) - 100% TPR with 0% FPR\n",
    "- The **area under this curve (AUC)** summarizes the overall performance\n",
    "\n",
    "**Advantages of ROC-AUC**:\n",
    "- Threshold-independent: Evaluates model performance across all possible thresholds, not just one.\n",
    "- Robust to class imbalance: Unlike accuracy, AUC is less affected by imbalanced datasets.\n",
    "- Single metric: Provides one number to compare models while considering the full range of operating points.\n",
    "- Probabilistic interpretation: Represents the probability that the model ranks a random positive example higher than a random negative example.\n",
    "- Visual insight: The ROC curve shows the trade-off between sensitivity and specificity.\n",
    "\n",
    "**Disadvantages of ROC-AUC**:\n",
    "- Can be overly optimistic with highly imbalanced data: May show good AUC even when precision is poor.\n",
    "- Doesn't reflect real-world threshold: You still need to choose a threshold for actual predictions.\n",
    "- Less informative for imbalanced datasets: Precision-Recall curves may be more appropriate in such cases.\n",
    "- Doesn't show prediction calibration: High AUC doesn't mean probabilities are well-calibrated.\n",
    "\n",
    "**When should ROC-AUC be used?**\n",
    "- When you need to evaluate model performance across **all possible thresholds**.\n",
    "- When you want a **single metric** that's **robust to class imbalance** (though with caveats).\n",
    "- When **both classes are important** and you want to balance TPR and FPR.\n",
    "- For **model comparison** when you haven't decided on an operating threshold yet.\n",
    "- In scenarios like **ranking problems** where relative ordering matters more than absolute predictions.\n",
    "\n",
    "**Example Use Cases**:\n",
    "- Medical screening where different thresholds may be used depending on resource availability\n",
    "- Credit scoring where the threshold can be adjusted based on business needs\n",
    "- Spam detection where the trade-off between catching spam and blocking legitimate emails can vary\n",
    "- Fraud detection where the sensitivity can be tuned based on risk tolerance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using ROC Curves to Find the Optimal Threshold\n",
    "\n",
    "While the AUC score tells us about overall model performance, the ROC curve can also help us **select the best threshold** for our specific use case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__`Step 30a`__ Find the optimal threshold using the **Youden's J statistic** (maximizes TPR - FPR)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Youden's J statistic for each threshold\n",
    "# J = TPR - FPR = Sensitivity - (1 - Specificity)\n",
    "youdens_j = tpr - fpr\n",
    "\n",
    "# Find the optimal threshold (maximum J)\n",
    "optimal_idx = np.argmax(youdens_j)\n",
    "optimal_threshold_roc = thresholds[optimal_idx]\n",
    "\n",
    "print(f\"Optimal threshold (ROC - Youden's J): {optimal_threshold_roc:.4f}\")\n",
    "print(f\"TPR at optimal threshold: {tpr[optimal_idx]:.4f}\")\n",
    "print(f\"FPR at optimal threshold: {fpr[optimal_idx]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__`Step 30b`__ Visualize the optimal threshold on the ROC curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot ROC curve with optimal threshold marked\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, color='blue', lw=2, label=f'ROC curve (AUC = {auc_val:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='gray', lw=2, linestyle='--', label='Random Classifier')\n",
    "\n",
    "# Mark the optimal threshold\n",
    "plt.scatter(fpr[optimal_idx], tpr[optimal_idx], color='red', s=100, \n",
    "            label=f'Optimal Threshold = {optimal_threshold_roc:.2f}', zorder=5)\n",
    "\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate', fontsize=12)\n",
    "plt.ylabel('True Positive Rate (Recall)', fontsize=12)\n",
    "plt.title('ROC Curve with Optimal Threshold', fontsize=14)\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.grid(alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"pr-curve\">\n",
    "    \n",
    "### 2.7. Precision-Recall Curve\n",
    "\n",
    "</a>\n",
    "\n",
    "While the ROC curve is useful in many scenarios, it can be **overly optimistic** when dealing with **highly imbalanced datasets**. In such cases, the **Precision-Recall (PR) Curve** provides a more informative picture of model performance.\n",
    "\n",
    "#### What is the Precision-Recall Curve?\n",
    "\n",
    "The PR curve plots:\n",
    "- **Y-axis**: Precision = $\\frac{TP}{TP + FP}$\n",
    "- **X-axis**: Recall = $\\frac{TP}{TP + FN}$\n",
    "\n",
    "By varying the classification threshold from 0 to 1, we get different combinations of precision and recall, which create the PR curve.\n",
    "\n",
    "#### Why use PR Curves for imbalanced data?\n",
    "\n",
    "When you have **imbalanced classes** (e.g., 1% positive, 99% negative):\n",
    "- ROC curves can be misleading because a high number of true negatives (TN) can make the False Positive Rate look artificially low\n",
    "- PR curves focus only on the positive class, making them more sensitive to model performance on the minority class\n",
    "- A poor model can still have a good-looking ROC curve with imbalanced data, but will show poor performance on a PR curve\n",
    "\n",
    "#### Interpreting the PR Curve\n",
    "\n",
    "- **Baseline**: A random classifier on an imbalanced dataset has a PR curve close to the proportion of positive samples (e.g., 0.1 if 10% are positive)\n",
    "- **Better models**: The curve is closer to the top-right corner (high precision and high recall)\n",
    "- **Perfect classifier**: Would have a point at (1, 1) - 100% precision and 100% recall\n",
    "- **Area Under PR Curve (AP)**: Can be computed as a summary metric, similar to AUC for ROC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<a href='https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_recall_curve.html'>sklearn.metrics.precision_recall_curve(y_true, probas_pred, ...)</a>\n",
    "\n",
    "__Definition:__ <br>\n",
    "Compute precision-recall pairs for different probability thresholds.\n",
    "\n",
    "__Returns:__ <br>\n",
    "_precision_: Precision values; <br>\n",
    "_recall_: Recall values; <br>\n",
    "_thresholds_: Increasing thresholds on the decision function used to compute precision and recall.\n",
    "\n",
    "__Parameters:__ <br>\n",
    "_y_true_: Ground truth (correct) target values; <br>\n",
    "_probas_pred_: Target scores (probability estimates of the positive class); <br>\n",
    "...\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__`Step 31`__ Compute and plot the Precision-Recall curve for the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Precision-Recall curve\n",
    "precision, recall, thresholds_pr = precision_recall_curve(y_val, y_pred_proba_val)\n",
    "\n",
    "# Calculate the baseline (proportion of positive samples)\n",
    "baseline = y_val.sum() / len(y_val)\n",
    "\n",
    "# Plot Precision-Recall curve\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(recall, precision, color='blue', lw=2, label='PR curve')\n",
    "plt.axhline(y=baseline, color='gray', linestyle='--', lw=2, label=f'Baseline (Random) = {baseline:.2f}')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('Recall', fontsize=12)\n",
    "plt.ylabel('Precision', fontsize=12)\n",
    "plt.title('Precision-Recall Curve', fontsize=14)\n",
    "plt.legend(loc=\"best\")\n",
    "plt.grid(alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Understanding the Precision-Recall Curve:**\n",
    "- The **horizontal dashed line** represents the baseline (proportion of positive samples in the dataset)\n",
    "- **Higher curves** (closer to top-right) indicate better model performance\n",
    "- The curve typically shows a **trade-off**: as recall increases, precision tends to decrease\n",
    "- Unlike ROC curves, there's often a **\"sawtooth\" pattern** due to changes in the ranking of predictions\n",
    "\n",
    "**Advantages of Precision-Recall Curves**:\n",
    "- Better for imbalanced datasets: More informative than ROC when the positive class is rare.\n",
    "- Focuses on positive class: Directly shows performance on the class of interest.\n",
    "- Sensitive to improvements: Better reflects improvements in model performance on minority class.\n",
    "- No influence from TN: Not affected by the large number of true negatives in imbalanced datasets.\n",
    "- Practical for many applications: Directly shows the precision-recall trade-off you'll face in deployment.\n",
    "\n",
    "**Disadvantages of Precision-Recall Curves**:\n",
    "- Less intuitive: Not as well-known or easily interpreted as ROC curves.\n",
    "- Baseline changes with data: The random baseline varies with class imbalance, making comparison across datasets harder.\n",
    "- No information on TN: Completely ignores true negatives, which may be important in some applications.\n",
    "- Harder to compare: Visual comparison of curves can be more difficult than with ROC curves.\n",
    "\n",
    "**When should Precision-Recall Curves be used?**\n",
    "- When dealing with **highly imbalanced datasets** (e.g., fraud detection, rare disease diagnosis).\n",
    "- When the **positive class is more important** than the negative class.\n",
    "- When you need to **understand the precision-recall trade-off** for threshold selection.\n",
    "- When **true negatives are abundant** and not particularly informative.\n",
    "- In scenarios where both **precision and recall matter** for the positive class.\n",
    "\n",
    "**Example Use Cases**:\n",
    "- Fraud detection (1% fraudulent transactions, 99% legitimate)\n",
    "- Medical diagnosis for rare diseases (1% disease prevalence)\n",
    "- Information retrieval (finding relevant documents in a large corpus)\n",
    "- Anomaly detection (rare events in system monitoring)\n",
    "- Click-through rate prediction (small percentage of users click)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Precision-Recall Curves to Find the Optimal Threshold\n",
    "\n",
    "The Precision-Recall curve is especially useful for finding the optimal threshold when dealing with imbalanced datasets or when precision and recall have different importance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__`Step 31a`__ Find the optimal threshold by maximizing the **F1 score** (balances precision and recall)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate F1 score for each threshold\n",
    "# F1 = 2 * (precision * recall) / (precision + recall)\n",
    "f1_scores = np.where((precision + recall) == 0, 0, 2 * (precision * recall) / (precision + recall))\n",
    "\n",
    "# Find the optimal threshold (maximum F1)\n",
    "optimal_idx_pr = np.argmax(f1_scores)\n",
    "optimal_threshold_pr = thresholds_pr[optimal_idx_pr]\n",
    "\n",
    "print(f\"Optimal threshold (PR - Max F1): {optimal_threshold_pr:.4f}\")\n",
    "print(f\"Precision at optimal threshold: {precision[optimal_idx_pr]:.4f}\")\n",
    "print(f\"Recall at optimal threshold: {recall[optimal_idx_pr]:.4f}\")\n",
    "print(f\"F1 Score at optimal threshold: {f1_scores[optimal_idx_pr]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__`Step 31b`__ Visualize the optimal threshold on the Precision-Recall curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot PR curve with optimal threshold marked\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(recall, precision, color='blue', lw=2, label='PR curve')\n",
    "plt.axhline(y=baseline, color='gray', linestyle='--', lw=2, label=f'Baseline = {baseline:.2f}')\n",
    "\n",
    "# Mark the optimal threshold\n",
    "plt.scatter(recall[optimal_idx_pr], precision[optimal_idx_pr], color='red', s=100, \n",
    "            label=f'Optimal Threshold = {optimal_threshold_pr:.2f}\\n(Max F1 = {f1_scores[optimal_idx_pr]:.2f})', \n",
    "            zorder=5)\n",
    "\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('Recall', fontsize=12)\n",
    "plt.ylabel('Precision', fontsize=12)\n",
    "plt.title('Precision-Recall Curve with Optimal Threshold', fontsize=14)\n",
    "plt.legend(loc=\"best\")\n",
    "plt.grid(alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__`Step 31c`__ Compare the thresholds found by different methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a summary table of optimal thresholds\n",
    "threshold_methods = pd.DataFrame({\n",
    "    'Method': ['Default', 'ROC (Youden\\'s J)', 'PR (Max F1)'],\n",
    "    'Threshold': [0.5, optimal_threshold_roc, optimal_threshold_pr]\n",
    "})\n",
    "\n",
    "print(\"Optimal Thresholds by Different Methods:\")\n",
    "print(threshold_methods)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing Metrics\n",
    "\n",
    "Now that we've covered all the main classification metrics, let's consolidate them in a summary table."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__`Step 32`__ Consolidate all the classification metrics (accuracy, precision, recall, F1, and AUC) for the train and validation sets into a single table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_metrics = pd.DataFrame({\n",
    "    'Metric': ['Accuracy', 'Precision', 'Recall', 'F1', 'AUC'],\n",
    "    'Train': [accuracy_train, precision_train, recall_train, f1_train, auc_train],\n",
    "    'Validation': [accuracy_val, precision_val, recall_val, f1_val, auc_val]\n",
    "})\n",
    "\n",
    "classification_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"classification-comparison\">\n",
    "\n",
    "### 2.8. How Metrics Can Change Model Selection: A Classification Example\n",
    "\n",
    "</a>\n",
    "\n",
    "Just like in the regression section, let's build a simple numerical example to see how different classification metrics might lead us to prefer different models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__`Step 33`__ Define the true labels and three different sets of predictions representing competing models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true_cls = np.array([1, 0, 1, 0, 1, 0, 1, 0])\n",
    "\n",
    "# Model A: balanced performance, few mistakes\n",
    "preds_model_A_cls = np.array([1, 0, 1, 0, 1, 0, 0, 0])\n",
    "\n",
    "# Model B: very conservative, rarely predicts positive\n",
    "preds_model_B_cls = np.array([0, 0, 1, 0, 0, 0, 0, 0])\n",
    "\n",
    "# Model C: aggressive, predicts many positives\n",
    "preds_model_C_cls = np.array([1, 1, 1, 0, 1, 1, 1, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__`Step 34`__ Build a helper function that gathers the main classification metrics for any set of predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classification_metrics_summary(y_true, y_pred):\n",
    "    return [\n",
    "        accuracy_score(y_true, y_pred),\n",
    "        precision_score(y_true, y_pred, zero_division=0),\n",
    "        recall_score(y_true, y_pred, zero_division=0),\n",
    "        f1_score(y_true, y_pred, zero_division=0)\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__`Step 35`__ Use the helper to compare the models across accuracy, precision, recall, and F1, and display the results in a table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_comparison = pd.DataFrame({\n",
    "    'Metric': ['Accuracy', 'Precision', 'Recall', 'F1'],\n",
    "    'Model A': classification_metrics_summary(y_true_cls, preds_model_A_cls),\n",
    "    'Model B': classification_metrics_summary(y_true_cls, preds_model_B_cls),\n",
    "    'Model C': classification_metrics_summary(y_true_cls, preds_model_C_cls)\n",
    "})\n",
    "\n",
    "classification_comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__`Step 36`__ Reflect on the table and discuss which model each metric prefers and why.\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "### Analysis of the Results\n",
    "\n",
    "* **Model A** (balanced behaviour) delivers the **highest accuracy (0.875)** and **F1 score (0.86)** while keeping precision at 1.0. It is the most even trade-off when you value overall correctness and a balance between precision and recall.\n",
    "* **Model B** (conservative) achieves **perfect precision (1.0)** but sacrifices recall (0.25) and overall accuracy (0.625). It would be preferred only when false positives are extremely costly and missing positives is acceptable.\n",
    "* **Model C** (aggressive) reaches **perfect recall (1.0)** and a strong F1 score (0.80) but at the expense of precision (â‰ˆ0.67). This is ideal when missing a positive case is much worse than raising some false alarms.\n",
    "\n",
    "### Key Takeaway\n",
    "\n",
    "Different classification metrics can point to different \"best\" models. Always align the metric you optimise with the real-world cost of false positives and false negatives.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"multiclass\">\n",
    "\n",
    "# 3. Multiclass Classification (Extra)\n",
    "\n",
    "</a>\n",
    "\n",
    "So far, we've focused on **binary classification** (2 classes: 0 and 1). However, many real-world problems involve **multiclass classification** where there are **3 or more classes**. Examples include:\n",
    "- Classifying images of animals (cat, dog, bird, fish, etc.)\n",
    "- Predicting iris species (setosa, versicolor, virginica)\n",
    "- Recognizing handwritten digits (0-9)\n",
    "- Categorizing customer feedback (positive, neutral, negative)\n",
    "\n",
    "When dealing with multiclass problems, we need to adapt our binary metrics (precision, recall, F1) to handle multiple classes. In this section, we'll explore how to compute and interpret performance metrics for multiclass classification problems using a practical example.\n",
    "\n",
    "To demonstrate these concepts, we'll work with the **Iris dataset** throughout this section, implementing each metric as we learn about it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__`Step 37`__ Import the Iris dataset and prepare it for multiclass classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "X_iris = iris.data\n",
    "y_iris = iris.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__`Step 38`__ Split the data and train a multiclass logistic regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into train and test sets\n",
    "X_train_iris, X_test_iris, y_train_iris, y_test_iris = train_test_split(\n",
    "    X_iris, y_iris, test_size=0.3, random_state=42, stratify=y_iris\n",
    ")\n",
    "\n",
    "# Train a multiclass logistic regression\n",
    "iris_model = LogisticRegression(max_iter=200, random_state=42)\n",
    "iris_model.fit(X_train_iris, y_train_iris)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_iris = iris_model.predict(X_test_iris)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"multiclass-confusion\">\n",
    "\n",
    "### 3.1. Multiclass Confusion Matrix\n",
    "\n",
    "</a>\n",
    "\n",
    "For multiclass problems, the confusion matrix becomes a **K Ã— K matrix** where K is the number of classes. Unlike binary classification where we have a 2Ã—2 matrix, multiclass confusion matrices show all possible combinations of true and predicted classes.\n",
    "\n",
    "### Structure:\n",
    "\n",
    "The matrix has **K rows** (actual classes) and **K columns** (predicted classes):\n",
    "\n",
    "|                  | Predicted Class 0 | Predicted Class 1 | Predicted Class 2 |\n",
    "|------------------|-------------------|-------------------|-------------------|\n",
    "| **Actual Class 0** | Correct (TPâ‚€)     | Misclassified     | Misclassified     |\n",
    "| **Actual Class 1** | Misclassified     | Correct (TPâ‚)     | Misclassified     |\n",
    "| **Actual Class 2** | Misclassified     | Misclassified     | Correct (TPâ‚‚)     |\n",
    "\n",
    "- **Diagonal elements**: Correct predictions for each class\n",
    "- **Off-diagonal elements**: Misclassifications (shows which classes are confused with each other)\n",
    "- **Row sums**: Total actual instances of each class\n",
    "- **Column sums**: Total predicted instances of each class\n",
    "\n",
    "__`Step 39`__ Create and visualize the multiclass confusion matrix for the Iris predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create confusion matrix\n",
    "cm_iris = confusion_matrix(y_test_iris, y_pred_iris)\n",
    "\n",
    "# Visualize with a heatmap\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm_iris, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=iris.target_names,\n",
    "            yticklabels=iris.target_names,\n",
    "            cbar=True)\n",
    "plt.xlabel('Predicted Label', fontsize=12)\n",
    "plt.ylabel('True Label', fontsize=12)\n",
    "plt.title('Multiclass Confusion Matrix - Iris Dataset', fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Advantages of Multiclass Confusion Matrix**:\n",
    "- Complete picture: Shows all classification errors, not just overall accuracy\n",
    "- Error patterns: Reveals which classes are confused with each other\n",
    "- Per-class performance: Can see which classes the model struggles with\n",
    "- Foundation for metrics: Enables calculation of per-class precision, recall, and F1\n",
    "\n",
    "**Disadvantages of Multiclass Confusion Matrix**:\n",
    "- Can be large: With many classes, the matrix becomes difficult to visualize\n",
    "- No single number: Unlike accuracy, doesn't provide one metric for comparison\n",
    "- Requires interpretation: Need to analyze the matrix to understand performance\n",
    "\n",
    "**When should Multiclass Confusion Matrix be used?**\n",
    "- Always as a first step in multiclass evaluation\n",
    "- When you need to understand which specific classes are being confused\n",
    "- When diagnosing model errors for improvement\n",
    "- When different types of errors have different costs (e.g., medical diagnosis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"macro\">\n",
    "\n",
    "### 3.2. Macro-Averaged Metrics\n",
    "\n",
    "</a>\n",
    "\n",
    "When extending binary metrics (precision, recall, F1) to multiclass problems, we first need to calculate these metrics **for each class individually** using a **One-vs-Rest (OvR)** approach. Then we can average them.\n",
    "\n",
    "**Macro averaging** calculates the metric **independently for each class**, then takes the **simple (unweighted) mean** across all classes.\n",
    "\n",
    "### Formula:\n",
    "\n",
    "For K classes:\n",
    "\n",
    "$$\n",
    "\\text{Macro Metric} = \\frac{1}{K} \\sum_{i=1}^{K} \\text{Metric}_i\n",
    "$$\n",
    "\n",
    "For example:\n",
    "$$\n",
    "\\text{Macro Precision} = \\frac{1}{K} \\sum_{i=1}^{K} \\text{Precision}_i\n",
    "$$\n",
    "\n",
    "### One-vs-Rest (OvR) Approach:\n",
    "\n",
    "To calculate per-class metrics, we treat each class as a binary problem:\n",
    "- **Class i** is the positive class\n",
    "- **All other classes combined** are the negative class\n",
    "\n",
    "For each class, we can extract from the confusion matrix:\n",
    "- **TP**: Diagonal element for that class\n",
    "- **FP**: Sum of the column (excluding diagonal) - wrongly predicted as this class\n",
    "- **FN**: Sum of the row (excluding diagonal) - actually this class but predicted as others\n",
    "- **TN**: Everything else\n",
    "\n",
    "__`Step 40`__ Calculate macro-averaged precision, recall, and F1 score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate macro-averaged metrics\n",
    "macro_precision = precision_score(y_test_iris, y_pred_iris, average='macro')\n",
    "macro_recall = recall_score(y_test_iris, y_pred_iris, average='macro')\n",
    "macro_f1 = f1_score(y_test_iris, y_pred_iris, average='macro')\n",
    "\n",
    "print(\"Macro-Averaged Metrics:\")\n",
    "print(f\"Precision: {macro_precision:.4f}\")\n",
    "print(f\"Recall:    {macro_recall:.4f}\")\n",
    "print(f\"F1 Score:  {macro_f1:.4f}\")\n",
    "\n",
    "# Also show per-class metrics to understand the macro average\n",
    "print(\"\\nPer-Class Metrics (used to calculate macro average):\")\n",
    "prec_per_class = precision_score(y_test_iris, y_pred_iris, average=None)\n",
    "rec_per_class = recall_score(y_test_iris, y_pred_iris, average=None)\n",
    "f1_per_class = f1_score(y_test_iris, y_pred_iris, average=None)\n",
    "\n",
    "for i, class_name in enumerate(iris.target_names):\n",
    "    print(f\"{class_name:12} - Precision: {prec_per_class[i]:.4f}, Recall: {rec_per_class[i]:.4f}, F1: {f1_per_class[i]:.4f}\")\n",
    "\n",
    "print(f\"\\nMacro Precision = ({prec_per_class[0]:.4f} + {prec_per_class[1]:.4f} + {prec_per_class[2]:.4f}) / 3 = {macro_precision:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Advantages of Macro Averaging**:\n",
    "- Equal treatment: All classes are weighted equally, regardless of their frequency\n",
    "- Sensitive to minority classes: Poor performance on rare classes is not hidden\n",
    "- Simple interpretation: Just the average of per-class scores\n",
    "- Fair comparison: Good when all classes are equally important\n",
    "\n",
    "**Disadvantages of Macro Averaging**:\n",
    "- Ignores class imbalance: A class with 10 samples has the same weight as one with 1000 samples\n",
    "- Can be misleading: High macro score doesn't mean good overall performance if classes are imbalanced\n",
    "- May not reflect real-world importance: Some classes might be more important than others\n",
    "\n",
    "**When should Macro Averaging be used?**\n",
    "- When all classes are **equally important** to your problem\n",
    "- When you want to ensure the model performs well on **all classes**, including rare ones\n",
    "- In scenarios where **minority class performance matters** as much as majority class performance\n",
    "- When evaluating models on **balanced datasets**\n",
    "\n",
    "**Example Use Cases**:\n",
    "- Medical diagnosis where missing any disease (even rare ones) is critical\n",
    "- Quality control where all defect types need equal attention\n",
    "- Multi-label text classification where all labels are equally important"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"weighted\">\n",
    "\n",
    "### 3.3. Weighted-Averaged Metrics\n",
    "\n",
    "</a>\n",
    "\n",
    "**Weighted averaging** is similar to macro averaging, but it **weights each class by its frequency** (number of true samples). This accounts for class imbalance by giving more importance to classes with more samples.\n",
    "\n",
    "### Formula:\n",
    "\n",
    "$$\n",
    "\\text{Weighted Metric} = \\frac{1}{N} \\sum_{i=1}^{K} n_i \\times \\text{Metric}_i\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $N$ = total number of samples\n",
    "- $n_i$ = number of true samples in class $i$\n",
    "- $\\text{Metric}_i$ = metric (precision, recall, or F1) for class $i$\n",
    "\n",
    "This is equivalent to:\n",
    "$$\n",
    "\\text{Weighted Metric} = \\sum_{i=1}^{K} w_i \\times \\text{Metric}_i\n",
    "$$\n",
    "\n",
    "Where $w_i = \\frac{n_i}{N}$ is the proportion of samples in class $i$.\n",
    "\n",
    "__`Step 41`__ Calculate weighted-averaged precision, recall, and F1 score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate weighted-averaged metrics\n",
    "weighted_precision = precision_score(y_test_iris, y_pred_iris, average='weighted')\n",
    "weighted_recall = recall_score(y_test_iris, y_pred_iris, average='weighted')\n",
    "weighted_f1 = f1_score(y_test_iris, y_pred_iris, average='weighted')\n",
    "\n",
    "print(\"Weighted-Averaged Metrics:\")\n",
    "print(f\"Precision: {weighted_precision:.4f}\")\n",
    "print(f\"Recall:    {weighted_recall:.4f}\")\n",
    "print(f\"F1 Score:  {weighted_f1:.4f}\")\n",
    "\n",
    "# Show the calculation\n",
    "print(\"\\nHow weighted average is calculated:\")\n",
    "class_counts = np.bincount(y_test_iris)\n",
    "total_samples = len(y_test_iris)\n",
    "\n",
    "weighted_prec_manual = 0\n",
    "for i, class_name in enumerate(iris.target_names):\n",
    "    weight = class_counts[i] / total_samples\n",
    "    contribution = weight * prec_per_class[i]\n",
    "    weighted_prec_manual += contribution\n",
    "    print(f\"{class_name:12} - {class_counts[i]} samples ({weight:.3f} weight) Ã— {prec_per_class[i]:.4f} precision = {contribution:.4f}\")\n",
    "\n",
    "print(f\"\\nWeighted Precision = {weighted_prec_manual:.4f}\")\n",
    "\n",
    "# Compare with macro\n",
    "print(f\"\\nComparison:\")\n",
    "print(f\"Macro Precision:    {macro_precision:.4f} (equal weight to all classes)\")\n",
    "print(f\"Weighted Precision: {weighted_precision:.4f} (weighted by class frequency)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Advantages of Weighted Averaging**:\n",
    "- Accounts for class imbalance: More common classes have more influence on the score\n",
    "- More representative: Better reflects overall performance when classes have different sizes\n",
    "- Balances minority and majority: Not as extreme as micro (which ignores minority) or macro (which treats all equally)\n",
    "- Intuitive for imbalanced data: Matches real-world scenarios where some classes are naturally more common\n",
    "\n",
    "**Disadvantages of Weighted Averaging**:\n",
    "- Can hide minority class problems: Poor performance on rare classes has less impact\n",
    "- Not always aligned with goals: Sometimes rare classes are more important despite low frequency\n",
    "- Less interpretable: Not as straightforward as macro (simple average) or micro (overall accuracy)\n",
    "\n",
    "**When should Weighted Averaging be used?**\n",
    "- When you have **class imbalance** and want to account for it\n",
    "- When **class frequency reflects importance** (common classes are more important)\n",
    "- When you want a **balance** between treating all classes equally (macro) and focusing on overall performance (micro)\n",
    "- In real-world scenarios where class distribution in test data matches production data\n",
    "\n",
    "**Example Use Cases**:\n",
    "- Customer churn prediction (most customers don't churn, but both groups matter)\n",
    "- Fraud detection with natural imbalance (most transactions are legitimate)\n",
    "- Product categorization where some categories are naturally more common"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"micro\">\n",
    "\n",
    "### 3.4. Micro-Averaged Metrics\n",
    "\n",
    "</a>\n",
    "\n",
    "**Micro averaging** aggregates the contributions of **all classes** globally by first summing up the individual true positives, false positives, and false negatives across all classes, then calculating the metric.\n",
    "\n",
    "### Formula:\n",
    "\n",
    "$$\n",
    "\\text{Micro Precision} = \\frac{\\sum_{i=1}^{K} TP_i}{\\sum_{i=1}^{K} (TP_i + FP_i)}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{Micro Recall} = \\frac{\\sum_{i=1}^{K} TP_i}{\\sum_{i=1}^{K} (TP_i + FN_i)}\n",
    "$$\n",
    "\n",
    "**Important Property**: In multiclass classification, **Micro Precision = Micro Recall = Micro F1 = Accuracy**\n",
    "\n",
    "This happens because:\n",
    "- The sum of all TP across classes = total correct predictions\n",
    "- The sum of all (TP + FP) across classes = total predictions = total samples\n",
    "- The sum of all (TP + FN) across classes = total actual samples = total samples\n",
    "\n",
    "__`Step 42`__ Calculate micro-averaged metrics and verify they equal accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate micro-averaged metrics\n",
    "micro_precision = precision_score(y_test_iris, y_pred_iris, average='micro')\n",
    "micro_recall = recall_score(y_test_iris, y_pred_iris, average='micro')\n",
    "micro_f1 = f1_score(y_test_iris, y_pred_iris, average='micro')\n",
    "\n",
    "# Calculate accuracy for comparison\n",
    "accuracy = accuracy_score(y_test_iris, y_pred_iris)\n",
    "\n",
    "print(\"Micro-Averaged Metrics:\")\n",
    "print(f\"Precision: {micro_precision:.4f}\")\n",
    "print(f\"Recall:    {micro_recall:.4f}\")\n",
    "print(f\"F1 Score:  {micro_f1:.4f}\")\n",
    "print(f\"\\nAccuracy:  {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Advantages of Micro Averaging**:\n",
    "- Naturally weighted by class size: Larger classes have more influence\n",
    "- Equals accuracy: In multiclass, provides the same information as overall accuracy\n",
    "- Good for overall performance: Reflects the model's performance across all predictions\n",
    "- Simple interpretation: Just the proportion of correct predictions\n",
    "\n",
    "**Disadvantages of Micro Averaging**:\n",
    "- Dominated by majority classes: Performance on minority classes has minimal impact\n",
    "- Can be misleading with imbalance: Good micro score can hide poor minority class performance\n",
    "- Same as accuracy: Doesn't provide additional information beyond what accuracy already tells you\n",
    "- Ignores per-class importance: Treats all predictions equally regardless of class\n",
    "\n",
    "**When should Micro Averaging be used?**\n",
    "- When you care about **overall prediction accuracy** across all samples\n",
    "- When **larger classes are more important** and should dominate the metric\n",
    "- When you want a metric that **naturally weights by frequency**\n",
    "- In practice, **use accuracy instead** - it's the same and more interpretable!\n",
    "\n",
    "**Example Use Cases**:\n",
    "- General classification where overall accuracy is the primary concern\n",
    "- When class imbalance reflects real-world importance\n",
    "- Situations where micro averaging is mainly used: comparing it with macro/weighted to understand class imbalance effects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing All Averaging Methods\n",
    "\n",
    "Now that we've seen all three averaging methods, let's compare them side-by-side to understand when each is most appropriate.\n",
    "\n",
    "__`Step 43`__ Create a comprehensive comparison table of all averaging methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a comparison table\n",
    "comparison_data = {\n",
    "    'Averaging Method': ['Macro', 'Weighted', 'Micro', 'Accuracy'],\n",
    "    'Precision': [macro_precision, weighted_precision, micro_precision, '-'],\n",
    "    'Recall': [macro_recall, weighted_recall, micro_recall, '-'],\n",
    "    'F1 Score': [macro_f1, weighted_f1, micro_f1, '-'],\n",
    "    'Value': ['-', '-', '-', accuracy]\n",
    "}\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "print(\"Comparison of All Averaging Methods:\")\n",
    "print(comparison_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__`Step 44`__ Use `classification_report` to see all metrics in one comprehensive view."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The classification_report provides everything in one place\n",
    "print(classification_report(y_test_iris, y_pred_iris, target_names=iris.target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Takeaways for Multiclass Classification\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "**Summary of Multiclass Metrics:**\n",
    "\n",
    "1. **Multiclass Confusion Matrix (3.1)**:\n",
    "   - K Ã— K matrix showing all classification outcomes\n",
    "   - Diagonal = correct predictions, off-diagonal = confusions\n",
    "   - Essential first step to understand error patterns\n",
    "\n",
    "2. **Macro Averaging (3.2)**:\n",
    "   - Simple average of per-class metrics\n",
    "   - Treats all classes equally\n",
    "   - **Use when**: All classes are equally important\n",
    "\n",
    "3. **Weighted Averaging (3.3)**:\n",
    "   - Weighted average by class frequency\n",
    "   - Accounts for class imbalance\n",
    "   - **Use when**: Class frequency reflects importance\n",
    "\n",
    "4. **Micro Averaging (3.4)**:\n",
    "   - Global aggregation of TP, FP, FN\n",
    "   - Equals accuracy in multiclass\n",
    "   - **Use when**: Overall accuracy is the priority\n",
    "\n",
    "**Decision Tree for Choosing Averaging Method:**\n",
    "\n",
    "```\n",
    "Do all classes have equal importance?\n",
    "â”œâ”€ YES â†’ Use MACRO averaging\n",
    "â””â”€ NO â†’ Is class frequency aligned with importance?\n",
    "    â”œâ”€ YES â†’ Use WEIGHTED averaging\n",
    "    â””â”€ NO â†’ Consider MACRO or define custom weights\n",
    "    \n",
    "Want overall accuracy?\n",
    "â””â”€ Use MICRO averaging (or just accuracy)\n",
    "```\n",
    "\n",
    "**Practical Workflow:**\n",
    "1. Start with **confusion matrix** to see where errors occur\n",
    "2. Check **per-class metrics** (average=None) to identify problem classes\n",
    "3. Choose averaging method based on your business requirements:\n",
    "   - Equal class importance â†’ **macro**\n",
    "   - Natural class imbalance â†’ **weighted**\n",
    "   - Overall performance â†’ **micro** (accuracy)\n",
    "4. Use **classification_report** for comprehensive overview\n",
    "\n",
    "**Remember**: The choice of averaging method can significantly affect which model appears \"best\"!\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sources: <br>\n",
    "https://medium.com/human-in-a-machine-world/mae-and-rmse-which-metric-is-better-e60ac3bde13d <br>\n",
    "https://towardsdatascience.com/accuracy-precision-recall-or-f1-331fb37c5cb9"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machine_learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
